{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ FAISS index not found. Rebuilding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# ✅ Securely Load Hugging Face API Token\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if huggingface_token is None:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.\")\n",
    "\n",
    "# ✅ Use Open-Source LLM from HuggingFace\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    huggingfacehub_api_token=huggingface_token,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "# ✅ Define FAISS index file path\n",
    "FAISS_INDEX_PATH = \"faiss_index.bin\"\n",
    "\n",
    "# ✅ Check if FAISS index exists and load it if available\n",
    "if os.path.exists(FAISS_INDEX_PATH):\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    print(\"✅ FAISS index loaded from disk.\")\n",
    "else:\n",
    "    print(\"⚠️ FAISS index not found. Rebuilding...\")\n",
    "    \n",
    "    # ✅ Load Personal Documents\n",
    "    pdf_files = [\n",
    "        \"EngTranscript.pdf\",\n",
    "        \"myCollected_Certificate.pdf\",\n",
    "        \"Ponkrit_CV(Eng).pdf\",\n",
    "        \"myAIT_Application.pdf\"\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "    # ✅ Split documents into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # ✅ Extract text content from chunks\n",
    "    texts = [doc.page_content for doc in text_chunks]\n",
    "\n",
    "    # ✅ Convert text to embeddings using SentenceTransformer\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "    # ✅ Convert embeddings to numpy array for FAISS\n",
    "    embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    # ✅ Initialize FAISS index\n",
    "    index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "    index.add(embedding_matrix)\n",
    "\n",
    "    # ✅ Save FAISS index to disk\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "    print(\"✅ FAISS index saved to disk.\")\n",
    "\n",
    "# ✅ Create FAISS vector store\n",
    "docstore = InMemoryStore()\n",
    "index_to_docstore_id = {}\n",
    "\n",
    "document_objects = []\n",
    "for i, doc in enumerate(text_chunks):\n",
    "    doc_object = Document(page_content=doc.page_content, metadata=doc.metadata)\n",
    "    document_objects.append(doc_object)\n",
    "    index_to_docstore_id[i] = str(i)\n",
    "\n",
    "docstore.mset([(str(i), doc) for i, doc in enumerate(document_objects)])\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model.encode,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "# Fix: Override `docstore.search` with `mget()`\n",
    "def docstore_get(doc_id):\n",
    "    docs = docstore.mget([doc_id])\n",
    "    return docs[0] if docs else None\n",
    "\n",
    "vector_store.docstore.search = docstore_get\n",
    "\n",
    "# ✅ Setup Retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# ✅ Define the structured prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specializing in answering questions about Ponkrit Kaewsawee.\n",
    "Your responses should be precise, informative, and based only on the provided documents.\n",
    "If the requested information is unavailable, politely state that you don’t have enough data.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Set up LangChain RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# ✅ Function to ask chatbot questions\n",
    "def ask_chatbot(question):\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\", []\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    return response[\"result\"], response[\"source_documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "Transformers Version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
