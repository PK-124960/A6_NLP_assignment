{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m huggingface_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACEHUB_API_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Fix: Correct env variable name\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m huggingface_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ✅ Use Open-Source LLM from HuggingFace\u001b[39;00m\n\u001b[1;32m     23\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[1;32m     24\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39mhuggingface_token,  \u001b[38;5;66;03m# ✅ Pass the token explicitly\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m}  \u001b[38;5;66;03m# Adjust parameters as needed\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# ✅ Securely Load Hugging Face API Token\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # Fix: Correct env variable name\n",
    "if huggingface_token is None:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.\")\n",
    "\n",
    "# ✅ Use Open-Source LLM from HuggingFace\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    huggingfacehub_api_token=huggingface_token,  # ✅ Pass the token explicitly\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}  # Adjust parameters as needed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Load Personal Documents\n",
    "pdf_files = [\n",
    "    \"EngTranscript.pdf\",\n",
    "    \"myCollected_Certificate.pdf\",\n",
    "    \"Ponkrit_CV(Eng).pdf\",\n",
    "    \"myAIT_Application.pdf\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# ✅ Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# ✅ Extract text content from chunks\n",
    "texts = [doc.page_content for doc in text_chunks]\n",
    "\n",
    "# ✅ Convert text to embeddings using SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "# ✅ Convert embeddings to numpy array for FAISS\n",
    "embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# ✅ Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# ✅ Create FAISS vector store\n",
    "docstore = InMemoryStore()\n",
    "index_to_docstore_id = {}\n",
    "\n",
    "document_objects = []\n",
    "for i, doc in enumerate(text_chunks):\n",
    "    doc_object = Document(page_content=doc.page_content, metadata=doc.metadata)\n",
    "    document_objects.append(doc_object)\n",
    "    index_to_docstore_id[i] = str(i)\n",
    "\n",
    "# Store documents correctly\n",
    "docstore.mset([(str(i), doc) for i, doc in enumerate(document_objects)])\n",
    "\n",
    "# ✅ Ensure `mget()` is used for retrieval\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model.encode,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "# Fix: Override `docstore.search` with `mget()`\n",
    "def docstore_get(doc_id):\n",
    "    docs = docstore.mget([doc_id])\n",
    "    return docs[0] if docs else None\n",
    "\n",
    "vector_store.docstore.search = docstore_get  # ✅ Fix: Proper retrieval function\n",
    "\n",
    "# ✅ Setup Retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# ✅ Use Open-Source LLM from HuggingFace\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\")\n",
    "\n",
    "# ✅ Define the structured prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specializing in answering questions about Ponkrit Kaewsawee.\n",
    "Your responses should be precise, informative, and based only on the provided documents.\n",
    "If the requested information is unavailable, politely state that you don’t have enough data.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Set up LangChain RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# ✅ Function to ask chatbot questions\n",
    "def ask_chatbot(question):\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\", []\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    return response[\"result\"], response[\"source_documents\"]\n",
    "\n",
    "# ✅ Test with a sample question\n",
    "question = \"What is Ponkrit Kaewsawee's highest level of education?\"\n",
    "answer, sources = ask_chatbot(question)\n",
    "\n",
    "# ✅ Print answer and sources\n",
    "print(\"Answer:\", answer)\n",
    "for source in sources:\n",
    "    print(\"Source:\", source.metadata.get(\"source\", \"Unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "Transformers Version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
